# **Win Report: Glean Agent Builder Cohort (“Glean Cohort”) at Docebo**

## **Executive Summary**

The **Glean Cohort**—formally the **Glean Agent Builder Cohort (GABC)**—has become the engine behind Docebo’s internal AI adoption. It is a 5‑week certification program that trains domain experts to design, build, test, and deploy production‑ready **Glean agents** against real business workflows.

Key outcomes:

* **Participation & reach**

  * **22 participants** in the inaugural cohort from multiple departments (Support, SE, CS, Sales, Marketing, Product, RevOps, Enablement, etc.).  
  * **2 full cohorts launched** as of late 2025 (Cohort 1 complete; Cohort 2 in progress).  
  * **74 sign‑ups** overall across cohorts and waitlist, with **70 mapped** to specific cohorts/departments—demonstrating broad cross‑functional demand, heavily weighted toward GTM roles.  
* **What was provided**

  * A **structured 5‑week program** with weekly 1‑hour live workshops plus \~1.5 hours of independent work (≈2.5 hours per week per participant), focused on end‑to‑end agent lifecycle: problem scoping, agent design, build, testing, deployment, and ROI.  
  * Rich enablement assets: a dedicated **Glean collection** with 30+ items (decks, recordings, examples), pre/post surveys, agent PRD and ROI templates, Slack channels, office hours, and certification/badging in Lyceum.  
* **What came of it (program‑level impact)**

  * **Average 28.9% uplift in AI competencies** for Cohort 1, with **\+40.5%** in “ability to build basic agents in Glean” and **\+35%** in Glean platform familiarity.  
  * **NPS 9.73** for Cohort 1, driven by clarity of structure, hands‑on work, and real business use cases.  
  * **11 cohort‑built agents live across GTM teams** from Cohort 1 alone, feeding into an ecosystem of **140+ custom agents** and **180+ monthly agent users** company‑wide.  
* **Glean platform & business impact**

  * Glean now has **600+ monthly active users, \~70% company coverage**, and strong stickiness at **69% WAU/MAU**, with particularly high (\>80%) adoption in key GTM clusters (SE, Support, AEs, AMs, CX/PS, Product Management, Product Marketing).  
  * Flagship cohort‑linked agents (e.g., **Product FAQ, Renewal Plan Generator, Reach Scoring**) each show **500+ runs**, with Product FAQ alone reaching **600 runs by 90 users**, **93% time reduction** per query (15 minutes → \<1 minute), **2 hours/week** saved per rep, **100 NPS**, and **94% task success / 96% grounding** in UAT.  
* **Spotlight – Natalie Yamakami and the Product FAQ Agent**

  * Natalie (Revenue Enablement) is a **certified Glean Agent Builder** and architect of the **Product FAQ Agent**, built through the Glean Cohort.  
  * Her agent has become the **gold‑standard case study** for Docebo’s governance model—held up in leadership forums as the “golden child” example of end‑to‑end agent development, testing, rollout, feedback collection, and impact storytelling.  
  * She is recognized in AI Ops alignment notes as a core **AI champion**, alongside Ron Murphy, for building multiple widely used agents and proving the power of the cohort model.

Overall, the Glean Cohort program has transformed AI Ops from a bottleneck into an **“agent factory”** that scales Glean Agents through trained builders embedded in the business.

---

## **Detailed Analysis**

### **1\. Program Overview and Design**

#### **1.1 What the Glean Cohort is**

The **Glean Agent Builder Cohort** is a **5‑week certification program** at Docebo that teaches employees to design, build, and deploy AI agents using the Glean platform. Participants commit \~**2.5 hours per week**: a **1‑hour live workshop** plus about **1.5 hours of asynchronous project work**, with optional office hours.

The program’s goals:

* **Upskill AI and agent design capability** – AI literacy, prompting, agent patterns, debugging, and impact measurement on Glean.  
* **Produce certified, production‑ready agents** that solve concrete internal processes (e.g., Product FAQ, Vendor Procurement, Integrations Finder, Virtual Mentor, Marketing Onboarding Agent).  
* **Establish governance and quality control** – cohort participants become **certified builders**, and their agents go through a submission/review process before being surfaced as “approved agents.”

The cohort is now explicitly codified in the **Glean Enablement Plan** as Track 2 of the Glean rollout, alongside core user training and hackathons.

#### **1.2 Cohort structure and content**

Each cohort follows a structured, progressive curriculum:

* **Week 1 – Discover, Define & Scope**

  * Introduce cohort structure, expectations, and pre‑survey.  
  * Define “what is a Glean agent” and align on agentic workflows vs. simple prompts.  
  * Draft problem statements and early PRDs for each participant’s agent.  
* **Week 2 – Agent design patterns**

  * Teach agent building blocks (input collection, search, reasoning, tool calls, branching, human‑in‑the‑loop steps).  
* **Week 3 – Build lab**

  * Hands‑on building in Glean, guided by example patterns like the Meeting Recap agent.  
* **Week 4 – Measurement, UAT, deployment**

  * Define performance metrics (task success, grounding, incident rate) and “good enough to ship” thresholds.  
  * Compare deployment channels: Glean chat, Slack, embedded entry points, APIs.  
* **Week 5 – Demo day**

  * Participants present their agents, receive peer feedback, and see cross‑functional examples.

The backbone is **weekly 60‑minute workshops** (largely Wednesdays, 09:00 ET), “half lecture, half lab,” with **interactive breakouts** every session.

#### **1.3 Program assets and support**

What was provided to participants and the wider org:

* **Central Glean collection** – “Glean Agent Builder Cohort” collection with all decks, PDFs, references, and examples; linked from the **Docebo Internal AI Hub** as the home for the curriculum.  
* **Slack channels** – `#glean-help` for announcements and questions, and a private cohort channel for logistics, reminders, and certification communication.  
* **Office hours & 1:1 support** – dedicated office hours and ad‑hoc help from AI Ops (including internship support) focused on debugging, best practices, and use case design.  
* **Surveys & forms** – pre and post surveys to benchmark skill and NPS; mid‑cohort **Agent Information Collection** form to document each agent’s pitch, impact hypothesis, and URL.  
* **Certification & recognition** – **Lyceum “Certified Glean Agent Builder” badges** and Slack‑based company‑wide recognition of graduates and their agents.

Net: the program provided **not just training**, but a full **scaffold for agent productization**: curriculum, governance path, recognition, and an internal community of practice.

---

### **2\. Participation and Reach**

#### **2.1 Cohort count and timeline**

* **Cohort 1** – “Fall 2025” cohort (Sept–Oct), 5 weeks of workshops; this cohort is fully complete and has a consolidated Impact & Insights report.  
* **Cohort 2** – “Winter 2025” cohort (Nov–Dec), with similar structure, launched via a separate invitation form and set of calendar invites; Cohort 2 was in progress as of Dec 2025\.

#### **2.2 Attendees and sign‑ups**

To answer “how many attendees,” it’s useful to distinguish **interest**, **participants**, and **graduates**:

* **Interest / sign‑ups**

  * The **Glean Agent Builder Cohort sign‑up** sheet records **74 total responses**.  
  * A pivot tab (“Dept per Cohort”) shows **70 respondents** mapped to specific cohorts, departments, or waitlist—this is the practical “top of funnel” for cohort placement.  
* **Participants (Cohort 1\)**

  * The Impact & Insights report states that the inaugural Glean Cohort was a **5‑week upskilling series with 22 participants from different departments across the organization**.  
  * A CIO Q3 deck likewise highlights “**22 Participants – Agent Builder Cohort**” as a key AI initiative.  
* **Certified graduates (Cohort 1\)**

  * The Slack announcement of cohort completion lists **11 certified builders** who “finished the entire program and submitted their program,” including **natalie.yamakami@docebo.com**.  
* **Cohort 2 attendees**

  * Cohort 2 has clear invitations and survey handles but, as of the latest docs, does not yet have a consolidated impact report with final participant and graduation numbers. It is safe to say that **a second cohort of similar planned size (\~20–25) has run**, but final attendance is still being aggregated in analytics and retros.  
* **Department mix**

   From the sign‑up “Dept per Cohort” pivot (70 records), the interest distribution is:

  * Support: 16  
  * Solutions Engineering: 13  
  * Customer Success: 9  
  * Sales: 8  
  * Marketing: 7  
  * Product Management & Design: 7  
  * Revenue Operations: 4  
  * Enablement: 3  
  * IT: 1  
  * HR: 1  
  * Partnerships: 1  
  * Professional Services: 1  
* This shows that **well over half of sign‑ups were GTM‑facing roles**, which is exactly where Glean Agents can impact revenue and customer outcomes most directly.

#### **2.3 Engagement & completion signals**

* **Surveys**

  * Cohort 1 pre‑survey: **20 responses** out of 22 participants (91% coverage).  
  * Post‑survey: **11 responses** (50% coverage) – this is a completion proxy but not a precise attendance measure.  
* **Agent submissions**

  * Mid‑program, Slack records show that only **2 participants** had completed their final agent submission, prompting follow‑up reminders—evidence that **end‑of‑program documentation** was a friction point.

Net: Cohort 1 had **22 participants** with strong interest from GTM teams; 11 fully certified graduates; a second similar‑sized cohort has run; and total interest (74 sign‑ups) shows demand that exceeds initial seat capacity.

---

### **3\. What Was Provided: Curriculum, Assets, and Experience**

Beyond the high‑level description, the Glean Cohort provided several types of value:

#### **3.1 Skills and knowledge**

Participants were given guided practice in:

* **LLM prompting and agent thinking** – understanding how to give structured instructions, build reusable prompts, and reason about agent behavior.  
* **Agent design and implementation in Glean** – mapping business processes to agent steps, composing search \+ reasoning \+ tool steps, and managing context and memory.  
* **Measurement and business impact** – building ROI hypotheses, defining Task Success Rate / Grounding Rate, and using pilot data to tell a credible impact story.

Survey results show that competencies in **Glean platform familiarity** and **“build basic agents”** saw the largest improvements, which is consistent with this skill focus.

#### **3.2 Assets and support structures**

Participants and the org got:

* **Reusable templates** for agent PRDs, business hypotheses, and ROI framing.  
* **Agent catalog entries**, via the Agent Information Collection form, which later feed into the AI Hub / Agent catalog in Glean.  
* **Lyceum badges** and recognition, which help build a visible builder community and encourage continued engagement.

#### **3.3 Feedback from participants**

Cohort 1’s Impact & Insights and post‑survey responses highlight:

* Perceived **improvement in ability to apply AI to real business challenges**, not just theoretical understanding.  
* Strong appreciation for **interactive breakouts, peer collaboration, and demo day**, which participants saw as essential for understanding “what good looks like” for Glean agents.  
* High marks for **clarity of structure and documentation**, with the central Glean collection and Slack support frequently cited as enablers.

Improvements noted:

* Desire for **more in‑session build time** versus lecture.  
* Better calibration for **mixed experience levels**, potentially via self‑paced AI fundamentals before the cohort.

---

### **4\. What Came of It: Outcomes and Impact**

#### **4.1 Skill and adoption outcomes**

Cohort 1’s quantified learning outcomes:

* **Average 28.9% uplift** across AI competencies.  
* **\+40.5%** in ability to build basic agents in Glean (3.3 → 4.64).  
* **\+35.0%** in Glean platform familiarity (3.3 → 4.45).  
* **NPS 9.73**, with participants citing clarity, collaboration, and real‑world applicability as top drivers.

These data points show that the cohort did not just produce one‑off agents; it created a **distributed builder capability** across functions.

#### **4.2 Agent ecosystem and usage**

At the ecosystem level:

* AI Ops’ win summary reports **\~140+ custom agents**, **180+ monthly agent users**, and **140+ shared agents** used heavily by GTM roles.  
* By Q4 2025, Glean had **223 weekly active** and **275 monthly active** users on Mark’s GTM org alone; company‑wide MAU is **600+** with \~**70% coverage** and strong stickiness at **69% WAU/MAU**.

Cohort‑linked agents:

* The **Glean AI Impact Report** attributes to the cohorts **11 live agents across GTM teams from Cohort 1**, with **Cohort 2 in progress**.  
* High‑volume agents like **Product FAQ, Renewal Plan Generator, and Reach Scoring** each show **500+ runs**, demonstrating sustained use rather than novelty effects.

While not every agent came directly from the cohort, leadership decks and operating‑model docs explicitly state that **many of the most impactful GTM agents emerged organically from builder cohorts and champions**.

#### **4.3 Concrete business impact**

At a Glean program level, POC and impact documentation show:

* **30–75 minute reductions** in call and QBR prep time, and **5–9 hours per week saved** for heavy users in CS, Sales, and Support, when using Glean (search, chat, and agents) to assemble information across Salesforce, Gong, Drive, Slack, etc.  
* QBR‑related agents (e.g., account briefs, digital BR) targeting **60% reductions** in prep time (90 minutes → \<30 minutes).

The **Product FAQ Agent**—built and owned by Natalie—provides the clearest, fully quantified case:

* **93% reduction in search time** per question (15 minutes → \<1 minute), saving **\~2 hours/week** for reps.  
* **600 runs by 90 users** over \~two months; third most‑used agent in the org.  
* **Quality metrics**: 94% Task Success, 96% Grounding, 100 NPS in pilot.  
* Strong, unsolicited user quotes: “**extremely beneficial with getting quick, accurate information**,” “**helped me answer a question on a call in real‑time… would have taken \~30 minutes**,” and “**straight fire**.”

Behavioral impact:

* Glean usage data shows a **shift to assistant‑first behaviors**, with users now averaging **\~5 chats per week vs. 2.7 keyword searches**, indicating that AI‑assisted flows are becoming primary.  
* POC decks documented a **“search‑first workflow replacing Slack message digging”** and **fewer Slack interruptions for SMEs**, which the Product FAQ Agent and other cohort‑driven agents have amplified.

In other words: the Glean Cohort didn’t just train people; it **materially changed how knowledge work happens**, especially in GTM.

#### **4.4 Strategic follow‑ons**

The success of the cohort program led directly to:

* **Formalization in OKRs** – Q3/Q4 AI team OKRs commit to **launching and scaling the Agent Builder Cohort**, certifying 10–20 builders, graduating 80% of participants, and launching 10+ production agents scoring at least 7/10 on an impact framework.  
* **Embedding in the Glean upgrade strategy** – the Glean upgrade proposal treats **recurring cohorts** as a key lever for justifying and absorbing an enterprise‑level Glean investment (targets: 10+ certified builders across 6+ departments, 50+ active agents, 10+ ROI‑backed workflows).  
* **Operating‑model shift** – AI Ops’ “Agent Factory” narrative uses the cohort as proof that a builder‑multiplication strategy can launch **12+ new agents in a single quarter** and move AI Ops from builder of everything to **pattern‑setter and accelerator**.

Governance:

* In the Jan 27 governance session, the team agreed to **use the Glean cohort program as the gate for certified builders and approved agents**, while also recognizing that more robust testing and documentation are needed to ensure consistent trust in all “approved” agents.

---

### **5\. Spotlight: Natalie Yamakami and the Product FAQ Agent**

You requested specific emphasis on **natalie.yamakami@docebo.com** and her agents. She is, in practice, the **archetypal success story of the Glean Cohort**.

#### **5.1 Natalie’s role and context**

* **Role**: Revenue Enablement (Enablement within Revenue), reporting to **Jordyn Tavares**.  
* **Context**: Responsible for equipping the revenue org with effective product knowledge, playbooks, and tools.

Before and during the cohort, Natalie authored an archival concept doc, **“Glean Chatbot for Product Knowledge”**, specifying goals, sources, and feedback loops for an AI assistant that could answer product questions for Sales, BD, CS, SE, and PS. That doc pre‑figured the Product FAQ Agent she ultimately built via the cohort.

She signaled early enthusiasm in `#glean-help` when the cohort was announced (“**So hype about this\!\!\!**”).

#### **5.2 Journey through the Glean Cohort**

* In the **Cohort 1 sign‑up form**, Natalie articulated her use case:  
   **“Revitalizing the product-specific FAQ process for sellers\! Enabling them to sell products effectively while limiting the amount of back-and-forth questions asked via Slack channels. Potential to even expand this further into pricing as well.”**  
* She actively participated in workshops (calendar guest lists and meeting notes list her explicitly) and engaged deeply on ROI and measurement, requesting the ROI template during Week 1\.  
* Natalie completed the full program and is named among the **11 certified builders** in the company‑wide completion announcement.

Her direct feedback on the program to the broader company:

“This was such a great program, brilliantly facilitated and designed by Sam\! I can't wait to see what the second cohort and future cohorts create.”

#### **5.3 The Product FAQ Agent: design, pilot, and impact**

**Design (via cohort)**

In the Glean Cohort Agent Information form, Natalie defined the **Product FAQ Agent** as:

* Audience: **Sales / broader Revenue team**.  
* Purpose: Provide **instant, authoritative answers** to common product questions in Glean Chat, citing trusted internal sources (Product Knowledge collections, enablement content, etc.).  
* Mechanism: When the agent cannot answer, it **logs the question to a spreadsheet** maintained by Enablement and Product, ensuring gaps are filled and the knowledge base improves over time.  
* Hypothesis: Reps are wasting \~**2 hours per week** searching fragmented sources, and SMEs are being repeatedly pulled into the same basic questions; the agent aims to **reduce search time by \~90%** and free SME time.

**Pilot and launch**

Natalie then:

* Recruited **10–15 pilot users** via the `#ai-in-docebo` channel with a clear CTA: “**STOP SEARCHING\! Join the Product FAQ Agent Pilot now\!** … The Enablement team has built the Product FAQ Agent in Glean to give you instant, accurate answers and save you hours every week.”  
* Clarified scope vs. non‑scope (not for deep debugging / hyper‑technical questions) and insisted that the agent be used as an **internal enablement tool** rather than an external‑facing source (“a tool to use internally … then a rep could interpret how they’d want to share externally”).

Following thorough UAT:

* Sam Adams publicly recognized her in the cohort channel:

   “I am blown away by her thoroughness in the UAT to make sure the agent was up-to-par with the use case requirements, and her amazing launch info to set up users for success. Great work, Natalie\!\!”

* You, Sam, responded:

   “Those are some big numbers\!\! Great work Natalie\!\!”

Her **launch announcement** framed the value crisply:

“Stop Searching, Start Selling: The Product FAQ Agent is LIVE\! … Our pilot proved the Agent reduces search time by 93% (that's \~2 hours saved per week\!).”

**Impact**

Natalie’s **Impact Report: Product FAQ Agent** documents:

* **93% reduction in search time** (15 minutes → \<1 minute on average).  
* **\~2 hours per week saved** for reps doing product Q\&A, based on pilot results.  
* **600 runs by 90 users** over about two months, making it **Docebo’s third most‑used agent**.  
* Trust metrics: **Task Success 94%**, **Grounding 96%**, **NPS 100** among pilot users.  
* Adoption skew: while initial target was AX sellers, the agent saw its **strongest pull from CX**, which accounted for **48.2% of users and 277 of 600 runs**, plus users from AEs, AMs, Support, SE, Renewals, and others.

Representative user quotes:

* “Extremely beneficial with getting quick, accurate information on product. The response time was \<1 minute in most cases and was very informative.”  
* “The agent helped me answer a question on a call in real-time… It would have likely taken about 30 minutes to find or source this answer from a human.”  
* “It was really helpful and accurate. This will be a valuable capability to arm folks with. … Straight fire.”

Even post‑launch, Natalie continues to treat the agent as a product: she seeks **usage exports from Glean Insights** to monitor performance and iterate, escalating when CSV downloads fail so that data‑driven iteration can continue.

#### **5.4 Exemplar status in governance and operating model**

In the Jan 27 governance meeting, your notes explicitly say:

“Sam highlighted Natalie's work as the 'golden child' example of proper agent development, testing, rollout, feedback collection, and results communication to leadership.”

In a broader AI Operations alignment, Sam Adams contrasted weak RevOps engagement with:

“the enablement team's success. He noted that Rev Ops had only one or two graduates from the Glean cohort, with Gustavo building a vendor analysis agent, while the enablement team (Ron and Natalie) had built multiple useful agents that were actively being used and had become champions for the AI Ops team.”

In other words:

* Natalie is **not just a participant**; she is one of the **clearest proof‑points** that the cohort model can produce:  
  * a credible domain‑owned agent,  
  * strong pilot and impact data,  
  * cross‑functional adoption, and  
  * a repeatable pattern for future agents.

She embodies the **“certified builder” vision** embedded in the Glean Cohort design.

---

### **6\. Gaps, Risks, and Next Opportunities**

From the available data and retros:

* **Data gaps**

  * For Cohort 2, we don’t yet have a consolidated impact report (participant counts, uplift, NPS, certified agents). This limits our ability to present a fully symmetric story vs. Cohort 1\.  
  * Agent ROI is well‑defined in hypotheses and select deep‑dives (e.g., Product FAQ), but **not yet systematically measured** for all cohort‑built agents.  
* **Governance**

  * The governance session notes that, while the Glean cohort program currently certifies builders and agents, **not all approved agents are equally trusted**, and more structured testing, documentation, and lifecycle management (including deprecation) is needed.  
* **Adoption skew**

  * Adoption is strongest in GTM—especially CX, Sales, SE, Enablement—and weaker in RevOps and some operational teams.  
  * Future cohorts could be intentionally weighted toward under‑represented orgs, pairing them with champions like Natalie to co‑design agents.  
* **Experience improvements**

  * Participants asked for **more build time in sessions** and better support for mixed skill levels.  
  * Pre‑work modules for AI fundamentals and additional guided build labs could address this.

---

## **Conclusion**

The **Glean Cohort / Glean Agent Builder Cohort** has proven to be a **high‑leverage, repeatable mechanism** for turning Glean from “a search bar” into an **AI teammate embedded in revenue and customer workflows**.

On the **people side**, it has:

* Trained **22+ internal builders** with measurable skill uplift and an NPS of **9.73**.  
* Produced a visible community of **certified Glean Agent Builders**, recognized in Lyceum and internal comms.

On the **product side**, it has:

* Fueled an ecosystem of **140+ agents** and **180+ monthly agent users**, with GTM teams as the heaviest adopters.  
* Delivered flagship wins like the **Product FAQ Agent**, which alone achieved **93% time reduction**, **2 hours/week saved**, and **600 runs / 90 users with 100 NPS**.

On the **strategy and governance side**, it has:

* Become a cornerstone of the **Glean Enablement Plan** and AI OKRs, with explicit targets for certified builders, agents in production, and time‑to‑knowledge reduction.  
* Served as the de facto **governance gate** for agent approval and a model for how teams should develop and scale agents, with **Natalie Yamakami’s Product FAQ Agent** now codified as the canonical example.

From a win‑story perspective—especially for internal and external stakeholders—the narrative is strong:

**AI Ops built a Glean Cohort program that empowered domain experts like Natalie Yamakami to ship high‑impact agents. Those agents have reshaped how GTM teams access knowledge, reduced time‑to‑answer dramatically, and provided a scalable model for democratizing AI development across Docebo.**

Next steps to extend the win:

* **Scale**: Run additional cohorts targeted at under‑represented orgs (RevOps, Finance, HR), with explicit seat guarantees.  
* **Systematize ROI**: Standardize impact reporting for every certified agent (Product FAQ‑style dashboards and write‑ups).  
* **Harden governance**: Formalize testing, documentation, and deprecation standards, using Natalie’s end‑to‑end pattern as the template.  
* **Tell the story**: Package the Glean Cohort \+ Natalie/Product FAQ case as a flagship narrative for internal town halls, vendor references, and possibly external thought leadership.

If you’d like, I can next turn this into:

* a slide‑ready **exec deck**, or  
* a **one‑pager customer reference** describing how Docebo used Glean Cohorts and Agents internally to drive adoption and ROI.

